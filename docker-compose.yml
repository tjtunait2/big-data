version: '3.7'
services:
  spark_master:
    container_name: spark_master
    image: registry.gitlab.com/chung-pi/spark-zeppelin/spark_master:latest
    hostname: master
    environment:
      - ZEPPELIN_PORT=80
      - SPARK_MASTER=local[*]
      - MASTER=local[*]
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=2G
      - SPARK_DRIVER_MEMORY=512m
      - SPARK_EXECUTOR_MEMORY=512m
    healthcheck:
      disable: true
    dns:
      - 8.8.8.8
      - 8.8.4.4
    ports:
      - 4040:4040
      - 7077:7077
      - 6066:6066
      - 8081:8081
      - 80:80
      - 8080:8080
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "2"
    restart: "always"
    extra_hosts:
      - "master-spark:192.168.0.100"
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 4096M
        reservations:
          cpus: '1'
          memory: 4096M
    volumes:
      - './notebook:/usr/zeppelin/notebook'

  spark_worker_01:
    container_name: spark_worker_01
    image: spark_master:latest
    hostname: worker_01
    build:
      context: .
      dockerfile: Dockerfile.worker
      labels:
        com.bkwallet.description: "Big Data Class"
        com.bkwallet.maintainer: "ChungDT <chungdt@soict.hust.edu.vn>"
    environment:
      - ZEPPELIN_PORT=80
      - SPARK_MASTER=spark://master:7077
      - MASTER=spark://master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=128m
      - SPARK_EXECUTOR_MEMORY=256m
    healthcheck:
      disable: true
    dns:
      - 8.8.8.8
      - 8.8.4.4
    ports:
      - "8082:8081"
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "2"
    restart: "always"

  spark_worker_02:
    container_name: spark_worker_02
    image: spark_master:latest
    hostname: worker_02
    build:
      context: .
      dockerfile: Dockerfile.worker
      labels:
        com.bkwallet.description: "Big Data Class"
        com.bkwallet.maintainer: "ChungDT <chungdt@soict.hust.edu.vn>"
    environment:
      - ZEPPELIN_PORT=80
      - SPARK_MASTER=spark://master:7077
      - MASTER=spark://master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=128m
      - SPARK_EXECUTOR_MEMORY=256m
    healthcheck:
      disable: true
    dns:
      - 8.8.8.8
      - 8.8.4.4
    ports:
      - "8083:8081"
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "2"
    restart: "always"

  # rabbitmq:
  #   container_name: rabbitmq
  #   image: rabbitmq:3-management
  #   hostname: rabbitmq
  #   build:
  #     context: .
  #     dockerfile: Dockerfile.rabbitmq
  #     labels:
  #       com.bkwallet.description: "Big Data Class"
  #       com.bkwallet.maintainer: "ChungDT <chungdt@soict.hust.edu.vn>"
  #   environment:
  #     - SPARK_MASTER=spark://master:7077
  #     - MASTER=spark://master:7077
  #   healthcheck:
  #     disable: true
  #   dns:
  #     - 8.8.8.8
  #     - 8.8.4.4
  #   ports:
  #     - "15672:15672"
  #     - "5672:5672"
  #   logging:
  #     driver: json-file
  #     options:
  #       max-size: "50m"
  #       max-file: "2"
  #   restart: "always"

  # elasticsearch:
  #   image: docker.elastic.co/elasticsearch/elasticsearch:6.2.4
  #   container_name: elasticsearch
  #   environment:
  #     - cluster.name=docker-cluster
  #     - bootstrap.memory_lock=true
  #     - http.cors.enabled=true
  #     - http.cors.allow-origin=*
  #     - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
  #   build:
  #     context: .
  #     dockerfile: Dockerfile.elasticsearch
  #   ulimits:
  #     memlock:
  #       soft: -1
  #       hard: -1
  #   volumes:
  #     - './elasticsearch/esdata1:/usr/share/elasticsearch/data'
  #   ports:
  #     - 9200:9200

  # elasticsearch2:
  #   image: docker.elastic.co/elasticsearch/elasticsearch:6.2.4
  #   container_name: elasticsearch2
  #   build:
  #     context: .
  #     dockerfile: Dockerfile.elasticsearch
  #   environment:
  #     - cluster.name=docker-cluster
  #     - bootstrap.memory_lock=true
  #     - http.cors.enabled=true
  #     - http.cors.allow-origin=*
  #     - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
  #     - "discovery.zen.ping.unicast.hosts=elasticsearch"
  #   ulimits:
  #     memlock:
  #       soft: -1
  #       hard: -1
  #   volumes:
  #     - './elasticsearch/esdata2:/usr/share/elasticsearch/data'

  # kibana:
  #   image: 'docker.elastic.co/kibana/kibana:6.3.2'
  #   container_name: kibana
  #   environment:
  #     SERVER_NAME: kibana.local
  #     ELASTICSEARCH_URL: http://elasticsearch:9200
  #   ports:
  #     - '5601:5601'
# version: "3"

# services:
#   namenode:
#     image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
#     container_name: namenode
#     ports:
#       - 9870:9870
#       - 9000:9000
#     volumes:
#       - hadoop_namenode:/hadoop/dfs/name
#       - ./mnt:/mnt
#     environment:
#       - CLUSTER_NAME=test
#     env_file:
#       - ./hadoop.env

#   datanode1:
#     image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
#     container_name: datanode1
#     volumes:
#       - hadoop_datanode1:/hadoop/dfs/data
#     environment:
#       SERVICE_PRECONDITION: "namenode:9870"
#     env_file:
#       - ./hadoop.env
      
#   # datanode2:
#   #   image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
#   #   container_name: datanode2
#   #   volumes:
#   #     - hadoop_datanode2:/hadoop/dfs/data
#   #   environment:
#   #     SERVICE_PRECONDITION: "namenode:9870"
#   #   env_file:
#   #     - ./hadoop.env
      
#   # datanode3:
#   #   image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
#   #   container_name: datanode3
#   #   volumes:
#   #     - hadoop_datanode3:/hadoop/dfs/data
#   #   environment:
#   #     SERVICE_PRECONDITION: "namenode:9870"
#   #   env_file:
#   #     - ./hadoop.env
  
#   resourcemanager:
#     image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
#     container_name: resourcemanager
#     environment:
#       SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode1:9864"
#     env_file:
#       - ./hadoop.env

#   nodemanager1:
#     image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
#     container_name: nodemanager
#     environment:
#       SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode1:9864 resourcemanager:8088"
#     env_file:
#       - ./hadoop.env
  
#   historyserver:
#     image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
#     container_name: historyserver
#     environment:
#       SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode1:9864 resourcemanager:8088"
#     volumes:
#       - hadoop_historyserver:/hadoop/yarn/timeline
#     env_file:
#       - ./hadoop.env
  
# volumes:
#   hadoop_namenode:
#   hadoop_datanode1:
#   # hadoop_datanode2:
#   # hadoop_datanode3:
#   hadoop_historyserver:
